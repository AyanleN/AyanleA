{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction \u00b6 Author: Ayanle Noor Advisor: Tyson Lee Swetnam About \u00b6 Ayanle Noor is a rising Freshmen at University Of Arizona in Tucson, Arizona. During the summer of 2023 he is an intern at The University of Arizona through the BIO5 Institute's KEYS program. This summer he is an intern working at The University of Arizona in Dr. Tyson L. Swetnam's lab: a member of CyVerse , a cutting edge cyberinfrastructure funded by the National Science Foundation that is designed for research and committed to the principles of open science. This website follows the FAIR and CARE data principles and hopes to help further open science. Table of Contents \u00b6 Introduction Logbook Assignments Jupyter Notebooks GitHub Actions KEYS 2023 Main Project Results References GitHub Actions \u00b6 GitHub Education Access KEYS 2023 Main Project \u00b6 My Project Awesome Community Datasets Results \u00b6 Community Dataset Standardization Log Poster","title":"Introduction"},{"location":"#introduction","text":"Author: Ayanle Noor Advisor: Tyson Lee Swetnam","title":"Introduction"},{"location":"#about","text":"Ayanle Noor is a rising Freshmen at University Of Arizona in Tucson, Arizona. During the summer of 2023 he is an intern at The University of Arizona through the BIO5 Institute's KEYS program. This summer he is an intern working at The University of Arizona in Dr. Tyson L. Swetnam's lab: a member of CyVerse , a cutting edge cyberinfrastructure funded by the National Science Foundation that is designed for research and committed to the principles of open science. This website follows the FAIR and CARE data principles and hopes to help further open science.","title":"About"},{"location":"#table-of-contents","text":"Introduction Logbook Assignments Jupyter Notebooks GitHub Actions KEYS 2023 Main Project Results References","title":"Table of Contents"},{"location":"#github-actions","text":"GitHub Education Access","title":"GitHub Actions"},{"location":"#keys-2023-main-project","text":"My Project Awesome Community Datasets","title":"KEYS 2023 Main Project"},{"location":"#results","text":"Community Dataset Standardization Log Poster","title":"Results"},{"location":"Chatgpt/","text":"Prompting \u00b6 Prompting ChatGPT proved to be of utmost importance, as it provided the most valuable information for coding and utilizing the data I was working with. Regardless of the input character, ChatGPT made the best use of it and consistently delivered excellent responses. The only issue encountered was occasional mistakes in its output. However, by requesting corrections and finding ways to avoid repeating the same mistakes, the overall experience with ChatGPT was highly beneficial and productive. Google Bard \u00b6 Google Bard was the least useful; the code it generated had so many issues, and overall, it didn't work the best in my experience.\"","title":"ChatGPT"},{"location":"Chatgpt/#prompting","text":"Prompting ChatGPT proved to be of utmost importance, as it provided the most valuable information for coding and utilizing the data I was working with. Regardless of the input character, ChatGPT made the best use of it and consistently delivered excellent responses. The only issue encountered was occasional mistakes in its output. However, by requesting corrections and finding ways to avoid repeating the same mistakes, the overall experience with ChatGPT was highly beneficial and productive.","title":"Prompting"},{"location":"Chatgpt/#google-bard","text":"Google Bard was the least useful; the code it generated had so many issues, and overall, it didn't work the best in my experience.\"","title":"Google Bard"},{"location":"Leone/","text":"Result \u00b6 Figure 1: Exploration of Genetic Diversity using Second and Third Principal Components Figure 2: Unveiling Population-Specific Genetic Patterns: First, Second, and Third Principal Components Despite low variance Fig 1 visually represents sample relations based on the second and third principal components. It deviates from the true results, but effectively demonstrates what the first principal component would not emphasize. Africans and Asians (specifically, East Asians) show minimal changes prior to the first plot. In Fig 1 South Asians collided with Europeans, but in Fig 2 they experienced a significant shift. Americans, on the other hand, showed diverse representation with varied collision patterns across regions. These findings highlight the significance of the first principal component, which carries the majority of the variance.","title":"Result"},{"location":"Leone/#result","text":"Figure 1: Exploration of Genetic Diversity using Second and Third Principal Components Figure 2: Unveiling Population-Specific Genetic Patterns: First, Second, and Third Principal Components Despite low variance Fig 1 visually represents sample relations based on the second and third principal components. It deviates from the true results, but effectively demonstrates what the first principal component would not emphasize. Africans and Asians (specifically, East Asians) show minimal changes prior to the first plot. In Fig 1 South Asians collided with Europeans, but in Fig 2 they experienced a significant shift. Americans, on the other hand, showed diverse representation with varied collision patterns across regions. These findings highlight the significance of the first principal component, which carries the majority of the variance.","title":"Result"},{"location":"U/","text":"SOON....","title":"U"},{"location":"copilot/","text":"Copilot","title":"Copilot"},{"location":"cyverse/","text":"CyVerse \u00b6 Introduction \u00b6 CyVerse serves as a way for life scientists to share their data with others around the world through a common cyberinfrastructure. It is funded by the National Science Foudation's Directorate for Biological Science and is currently led by the University of Arizona. CyVerse's cyberinfrastructure allows scientists to store their data as well as share it with others through cloud computing for further analysis. Acting as a way to complete complex analyses and share large datasets, CyVerse furthers open science and enables a collaborative workspace. VICE Apps \u00b6 The Visual and Interactive Computing Environment(VICE) allows scientists to run interactive applications through CyVerse. Through this scientists can open their interactive applications(Jupyter Lab, RStudio, Shiny, WebGL, HTML5, VNC, and XPRA), transfer data into containters, analyze this data, and send their results to the cloud.","title":"CyVerse"},{"location":"cyverse/#cyverse","text":"","title":"CyVerse"},{"location":"cyverse/#introduction","text":"CyVerse serves as a way for life scientists to share their data with others around the world through a common cyberinfrastructure. It is funded by the National Science Foudation's Directorate for Biological Science and is currently led by the University of Arizona. CyVerse's cyberinfrastructure allows scientists to store their data as well as share it with others through cloud computing for further analysis. Acting as a way to complete complex analyses and share large datasets, CyVerse furthers open science and enables a collaborative workspace.","title":"Introduction"},{"location":"cyverse/#vice-apps","text":"The Visual and Interactive Computing Environment(VICE) allows scientists to run interactive applications through CyVerse. Through this scientists can open their interactive applications(Jupyter Lab, RStudio, Shiny, WebGL, HTML5, VNC, and XPRA), transfer data into containters, analyze this data, and send their results to the cloud.","title":"VICE Apps"},{"location":"githubactions/","text":"Introduction \u00b6 GitHub Actions allows for the automation of tasks within your software development life cycle. Through GitHub actions users can automatically run their software testing scripts. Key Vocabulary \u00b6 Workflows \u00b6 Workflows can be used to design, test, package, release, or deploy a project on GitHub. A workflow can be added to a repository in GitHub using the file name .github/workflows. Workflows consist of one or more jobs that are scheduled/triggered by an event. Events \u00b6 Events are the activities that trigger the start of a workflow. Workflows can be triggered using pus or pull requests. Runners \u00b6 Runners can be hosted by GitHub or own on your own and is basically considered as a server that has GitHub Actions runner application installed. A runner runs one job at a time and reports the progress to GitHub. Jobs \u00b6 A job is a sequence of instructions that run on the same runner. A workflow containing multiple jobs will perform them in parallel by default. You may also set up a pipeline to conduct jobs in a specific order. Steps \u00b6 A step is a single task that can be used to execute commands in a job. An action or a shell command can be used as a step. Each step of a task runs on the same runner, allowing all of the activities in that job to exchange data. Actions \u00b6 Actions are commands that are used to join steps to create a job. Actions can be created or found on the GitHub community. Figure credit : GitHub Docs The components of GitHub Actions that work together to run jobs","title":"Githubactions"},{"location":"githubactions/#introduction","text":"GitHub Actions allows for the automation of tasks within your software development life cycle. Through GitHub actions users can automatically run their software testing scripts.","title":"Introduction"},{"location":"githubactions/#key-vocabulary","text":"","title":"Key Vocabulary"},{"location":"githubactions/#workflows","text":"Workflows can be used to design, test, package, release, or deploy a project on GitHub. A workflow can be added to a repository in GitHub using the file name .github/workflows. Workflows consist of one or more jobs that are scheduled/triggered by an event.","title":"Workflows"},{"location":"githubactions/#events","text":"Events are the activities that trigger the start of a workflow. Workflows can be triggered using pus or pull requests.","title":"Events"},{"location":"githubactions/#runners","text":"Runners can be hosted by GitHub or own on your own and is basically considered as a server that has GitHub Actions runner application installed. A runner runs one job at a time and reports the progress to GitHub.","title":"Runners"},{"location":"githubactions/#jobs","text":"A job is a sequence of instructions that run on the same runner. A workflow containing multiple jobs will perform them in parallel by default. You may also set up a pipeline to conduct jobs in a specific order.","title":"Jobs"},{"location":"githubactions/#steps","text":"A step is a single task that can be used to execute commands in a job. An action or a shell command can be used as a step. Each step of a task runs on the same runner, allowing all of the activities in that job to exchange data.","title":"Steps"},{"location":"githubactions/#actions","text":"Actions are commands that are used to join steps to create a job. Actions can be created or found on the GitHub community. Figure credit : GitHub Docs The components of GitHub Actions that work together to run jobs","title":"Actions"},{"location":"githubed/","text":"GitHub Education \u00b6 Steps to Enroll \u00b6 Go to the GitHub Education Site and enter your education status as student From here your school email and dated documentation of your enrollment is required After this is approved you have access to the GitHub education student developer pack! What's Included and Functionality \u00b6 As part of the GitHub education student developer pack and GitHub global campus students and faculty are granted access to forums, Campus TV, exclusive events, and free software and subscriptions such as Canva Pro, Microsoft Azure, and VS code. note: the student developer pack doesn't include access to GitHub CodeSpaces","title":"GitHub Education Access"},{"location":"githubed/#github-education","text":"","title":"GitHub Education"},{"location":"githubed/#steps-to-enroll","text":"Go to the GitHub Education Site and enter your education status as student From here your school email and dated documentation of your enrollment is required After this is approved you have access to the GitHub education student developer pack!","title":"Steps to Enroll"},{"location":"githubed/#whats-included-and-functionality","text":"As part of the GitHub education student developer pack and GitHub global campus students and faculty are granted access to forums, Campus TV, exclusive events, and free software and subscriptions such as Canva Pro, Microsoft Azure, and VS code. note: the student developer pack doesn't include access to GitHub CodeSpaces","title":"What's Included and Functionality"},{"location":"google/","text":"Google bard","title":"Google"},{"location":"jupyter/","text":"Jupyter Notebooks \u00b6 What is it? \u00b6 seamless way to write and iterate python code to perform data analysis Notebooks allows code, figures, diagrams, charts, and explanations to all be stored in one location This allows developer's work to be shared, leading to collaboration and improving reproducibility Additionally, Jupyter Notebooks is free of charge, improving equity in data analysis and software development When new notebooks are created prebuilt docker containers are used to put the notebooks at their own path How does it work? \u00b6 The basis for Jupyter Notebooks is IPython, a command line shell for writing code in the terminal Juputer Notebooks allows this code to be written and iterated in the browser through the use of the ipykernel lines of code can be run all at once or one at a time Jupyter Notebooks supports mulitple languages, most common is python Allows storage of code and inclusion of Markdown files for notes and documentation when new notebooks are created prebuilt docker containers are used to place the notebooks on their own paths Structure \u00b6 Kernel \u00b6 the kernel is specific to the programming language, this project will be in python process that supports the notebook to execute the written code Juptyer team maintains the ipykernel, but other user maintained kernels are available for use Cell \u00b6 the cells are the main contents of the notebooks and are where code is written markdown cells can be created to store notes and info on code green = code running grey = code not running","title":"Jupyter Notebooks"},{"location":"jupyter/#jupyter-notebooks","text":"","title":"Jupyter Notebooks"},{"location":"jupyter/#what-is-it","text":"seamless way to write and iterate python code to perform data analysis Notebooks allows code, figures, diagrams, charts, and explanations to all be stored in one location This allows developer's work to be shared, leading to collaboration and improving reproducibility Additionally, Jupyter Notebooks is free of charge, improving equity in data analysis and software development When new notebooks are created prebuilt docker containers are used to put the notebooks at their own path","title":"What is it?"},{"location":"jupyter/#how-does-it-work","text":"The basis for Jupyter Notebooks is IPython, a command line shell for writing code in the terminal Juputer Notebooks allows this code to be written and iterated in the browser through the use of the ipykernel lines of code can be run all at once or one at a time Jupyter Notebooks supports mulitple languages, most common is python Allows storage of code and inclusion of Markdown files for notes and documentation when new notebooks are created prebuilt docker containers are used to place the notebooks on their own paths","title":"How does it work?"},{"location":"jupyter/#structure","text":"","title":"Structure"},{"location":"jupyter/#kernel","text":"the kernel is specific to the programming language, this project will be in python process that supports the notebook to execute the written code Juptyer team maintains the ipykernel, but other user maintained kernels are available for use","title":"Kernel"},{"location":"jupyter/#cell","text":"the cells are the main contents of the notebooks and are where code is written markdown cells can be created to store notes and info on code green = code running grey = code not running","title":"Cell"},{"location":"keysassignments/","text":"Keys Assignments \u00b6 Assignment 1: Internship Description \u00b6 During this summer, I have been given the exciting opportunity to work in Dr. Tyson L. Swetnam\u2019s Lab at the University of Arizona. Dr. Swetnam holds the position of research associate professor of Geoinformatics and is also a Joint faculty member in the school of natural resources and environment. His involvement with CyVerse, a platform focused on data science projects and promoting open science. Currently, Dr. Swetnam is actively engaged in a project aimed at maximizing the utilization of AI in scientific research. Project Description \u00b6 During my time in the lab, I will be contributing to a similar project that focuses on utilizing AI to construct a geographic map of genetic variation and the new Pangenome. The primary objective of this endeavor is to generate accessible data while simultaneously exploring how indigenous groups' data has been exploited within the scientific community. Assignment 2: Introduction to your Research \u00b6 Purpose: \u00b6 The primary objective is to maximize the utilization of the data we have gathered, and one of the most effective methods is through data visualization. By visualizing the data, we aim to not only gain valuable insights but also shed light on the exploitation of indigenous data within the scientific community. Previous Research: \u00b6 A substantial body of research has been conducted on the topic, providing a wealth of data that I will utilize in creating the geographic map. One notable example of such research is the 1000 Genomes Project, which extensively analyzed genetic variations among diverse individuals to enhance our understanding of human diversity. Leveraging the data obtained from this research, I will develop a comprehensive geographic map highlighting these variations. Need For Study: \u00b6 While the 1000 Genomes Project offers a substantial amount of data that I intend to leverage effectively, the question remains whether this dataset alone will be sufficient for my research project. Problem Statement: \u00b6 How can AI effectively handle vast amounts of data to optimize its utilization? AI systems often face challenges when analyzing large volumes of external data. In my work, I will explore strategies to harness the power of AI to its fullest potential, allowing me to leverage its capabilities and avoid the burden of manually processing extensive datasets. References \u00b6 https://www.internationalgenome.org/data-portal/population Assignment 3: Materials and Methods \u00b6 Assignment 4: Results \u00b6 Assignment 5 Long Abstract \u00b6 Assignment 6 Conclusion and Discussion \u00b6 Assignment 7 Title and Short Abstract \u00b6 Title \u00b6 Short Abstract \u00b6","title":"Assignments"},{"location":"keysassignments/#keys-assignments","text":"","title":"Keys Assignments"},{"location":"keysassignments/#assignment-1-internship-description","text":"During this summer, I have been given the exciting opportunity to work in Dr. Tyson L. Swetnam\u2019s Lab at the University of Arizona. Dr. Swetnam holds the position of research associate professor of Geoinformatics and is also a Joint faculty member in the school of natural resources and environment. His involvement with CyVerse, a platform focused on data science projects and promoting open science. Currently, Dr. Swetnam is actively engaged in a project aimed at maximizing the utilization of AI in scientific research.","title":"Assignment 1: Internship Description"},{"location":"keysassignments/#project-description","text":"During my time in the lab, I will be contributing to a similar project that focuses on utilizing AI to construct a geographic map of genetic variation and the new Pangenome. The primary objective of this endeavor is to generate accessible data while simultaneously exploring how indigenous groups' data has been exploited within the scientific community.","title":"Project Description"},{"location":"keysassignments/#assignment-2-introduction-to-your-research","text":"","title":"Assignment 2: Introduction to your Research"},{"location":"keysassignments/#purpose","text":"The primary objective is to maximize the utilization of the data we have gathered, and one of the most effective methods is through data visualization. By visualizing the data, we aim to not only gain valuable insights but also shed light on the exploitation of indigenous data within the scientific community.","title":"Purpose:"},{"location":"keysassignments/#previous-research","text":"A substantial body of research has been conducted on the topic, providing a wealth of data that I will utilize in creating the geographic map. One notable example of such research is the 1000 Genomes Project, which extensively analyzed genetic variations among diverse individuals to enhance our understanding of human diversity. Leveraging the data obtained from this research, I will develop a comprehensive geographic map highlighting these variations.","title":"Previous Research:"},{"location":"keysassignments/#need-for-study","text":"While the 1000 Genomes Project offers a substantial amount of data that I intend to leverage effectively, the question remains whether this dataset alone will be sufficient for my research project.","title":"Need For Study:"},{"location":"keysassignments/#problem-statement","text":"How can AI effectively handle vast amounts of data to optimize its utilization? AI systems often face challenges when analyzing large volumes of external data. In my work, I will explore strategies to harness the power of AI to its fullest potential, allowing me to leverage its capabilities and avoid the burden of manually processing extensive datasets.","title":"Problem Statement:"},{"location":"keysassignments/#references","text":"https://www.internationalgenome.org/data-portal/population","title":"References"},{"location":"keysassignments/#assignment-3-materials-and-methods","text":"","title":"Assignment 3: Materials and Methods"},{"location":"keysassignments/#assignment-4-results","text":"","title":"Assignment 4: Results"},{"location":"keysassignments/#assignment-5-long-abstract","text":"","title":"Assignment 5 Long Abstract"},{"location":"keysassignments/#assignment-6-conclusion-and-discussion","text":"","title":"Assignment 6 Conclusion and Discussion"},{"location":"keysassignments/#assignment-7-title-and-short-abstract","text":"","title":"Assignment 7 Title and Short Abstract"},{"location":"keysassignments/#title","text":"","title":"Title"},{"location":"keysassignments/#short-abstract","text":"","title":"Short Abstract"},{"location":"logbook/","text":"Logbook \u00b6 Day 1 (6/7) : Created a github website; there were some difficulties, but we learned by resolving the issues we had with the settings of my github account. Day 2 (6/8) : I worked on getting my bio published on my github page and editing it. Day 3 (6/9) I worked on the unix shell and git lessons we had, which went quite well. Day 4 (6/12) I worked on finding the data i need for my project, and using chatgpt made it easier to find a database of genetic variation. Day 5 (6/13) I worked on finding what part of the database i need for my project, had difficulty understanding what each section served. Day 6 (6/14) Today, i worked on getting an outline of what my project would look like and worked on finding database that works for my project using google and chatgpt. Day 7 (6/15) During the meeting, I received feedback on my project, which provided valuable insights on how to improve it. I carefully analyzed the data using Python and ChatGPT, and although it took a couple of minutes to compute, I was able to derive meaningful results. Additionally, I developed a blood type finder by combining individual data with external data sources to estimate a person's blood type accurately. Day 8 (6/16) I attended an orientation for the majority of the day. Once it concluded, I embarked on conducting research for my project. I discovered relevant data on https://www.internationalgenome.org and proceeded to analyze how I could effectively utilize this information. Day 9 (6/20) I have read a couple of papers and experimented with data using ChatGPT to create a map. It was successful for some data, but I encountered challenges when attempting to incorporate all of the data into a single place. Additionally, I have been exploring and experimenting with Jupyter Notebook for my projects Day 10 (6/21) I spent a considerable amount of time resolving an issue I encountered on GitHub. After working through it, I successfully fixed the problem and updated my website accordingly. As part of the update, I added a section to introduce the AI, ChatGPT, that I will be utilizing for my project. Additionally, I experimented with the data I had and encountered some challenges in determining the optimal way to leverage it effectively. Day 11 (6/22) After addressing the issue I encountered, I gained a better understanding of the project's structure and how to optimize the use of the available data. I proceeded to separate the data I would be utilizing from the data I wouldn't. Using Python, I extracted the longitude and latitude information from the dataset. Although I faced difficulties running it on Jupyter, I proceeded to organize the data and worked on analyzing the relevant genome data for determining its relatedness. Day 12 (6/23) I created a map to visualize the locations of the samples. I faced some challenges while using Jupyter as I couldn't directly import a CSV file into my code to obtain the longitude and latitude of the samples. However, I found a workaround by leveraging ChatGPT to help me generate the coordinates. With the map ready, I started examining the large dataset I'll be working with to determine the relationships within the data. The dataset is quite extensive, and it may take a few days to fully analyze. Fortunately, I used ChatGPT to guide me through the process, providing step-by-step instructions that made it easier for me to understand and follow along. Day 13 (6/26) I have created an account on CyVerse and uploaded a large amount of data that I will need to create the plot map I am aiming for. I have been working on resolving issues related to decompressing the VCF files into a readable format. Although I am still in the process of resolving this, I plan to complete it soon so that I can proceed with plotting the map. Day 14 (6/27) I have learned how to remove the VCF seal from a file and make the data readable. However, I am facing an issue with my computer's performance as it struggles to handle the entire file. Consequently, it takes about a minute or so for the file to open. Additionally, I recently learned the importance of not prematurely terminating a Jupyter lab session. Unfortunately, I made that mistake and ended up losing the downloaded data, which forced me to redownload everything. Day 15 (6/27) I worked on decompressing all of the files, but unfortunately most didn't decompress successfully which led me to not go through the pain of decompressing all, but usinhg cyvcf instead to read the files for me and decompress it. I went on to copy all of the files into my de.cyverse account and now plan to work on the results of the data. Day 16 (6/28) I worked on making the graphs, data is too large for Jupyter lab to generate. My code has had couple checks and it has been working but since the genetic file is too large it never generates the graph needed but loads forever, i plan to use only small part of the data. Day 17 (6/29) I finally got results, and plan to upadte all of my finding to the website this week and making my poster. Day 18 (7/3) Working on poster, and KEYS assignment! Day 19-22 (7/4-7) Previous result wasn't what i was looking for, there was lack of data that can be interrupted, so I worked to generate a data that people can interactive with and data that includes all samples rather than 100 or less. *Day 23-26 (7/10-13): Worked to fix the issue i had with the data and worked on KEYS assignment and updated my finding on my poster! Day 27 (7-14): Got the geographic map, and finished Working on my poster!","title":"Logbook"},{"location":"logbook/#logbook","text":"Day 1 (6/7) : Created a github website; there were some difficulties, but we learned by resolving the issues we had with the settings of my github account. Day 2 (6/8) : I worked on getting my bio published on my github page and editing it. Day 3 (6/9) I worked on the unix shell and git lessons we had, which went quite well. Day 4 (6/12) I worked on finding the data i need for my project, and using chatgpt made it easier to find a database of genetic variation. Day 5 (6/13) I worked on finding what part of the database i need for my project, had difficulty understanding what each section served. Day 6 (6/14) Today, i worked on getting an outline of what my project would look like and worked on finding database that works for my project using google and chatgpt. Day 7 (6/15) During the meeting, I received feedback on my project, which provided valuable insights on how to improve it. I carefully analyzed the data using Python and ChatGPT, and although it took a couple of minutes to compute, I was able to derive meaningful results. Additionally, I developed a blood type finder by combining individual data with external data sources to estimate a person's blood type accurately. Day 8 (6/16) I attended an orientation for the majority of the day. Once it concluded, I embarked on conducting research for my project. I discovered relevant data on https://www.internationalgenome.org and proceeded to analyze how I could effectively utilize this information. Day 9 (6/20) I have read a couple of papers and experimented with data using ChatGPT to create a map. It was successful for some data, but I encountered challenges when attempting to incorporate all of the data into a single place. Additionally, I have been exploring and experimenting with Jupyter Notebook for my projects Day 10 (6/21) I spent a considerable amount of time resolving an issue I encountered on GitHub. After working through it, I successfully fixed the problem and updated my website accordingly. As part of the update, I added a section to introduce the AI, ChatGPT, that I will be utilizing for my project. Additionally, I experimented with the data I had and encountered some challenges in determining the optimal way to leverage it effectively. Day 11 (6/22) After addressing the issue I encountered, I gained a better understanding of the project's structure and how to optimize the use of the available data. I proceeded to separate the data I would be utilizing from the data I wouldn't. Using Python, I extracted the longitude and latitude information from the dataset. Although I faced difficulties running it on Jupyter, I proceeded to organize the data and worked on analyzing the relevant genome data for determining its relatedness. Day 12 (6/23) I created a map to visualize the locations of the samples. I faced some challenges while using Jupyter as I couldn't directly import a CSV file into my code to obtain the longitude and latitude of the samples. However, I found a workaround by leveraging ChatGPT to help me generate the coordinates. With the map ready, I started examining the large dataset I'll be working with to determine the relationships within the data. The dataset is quite extensive, and it may take a few days to fully analyze. Fortunately, I used ChatGPT to guide me through the process, providing step-by-step instructions that made it easier for me to understand and follow along. Day 13 (6/26) I have created an account on CyVerse and uploaded a large amount of data that I will need to create the plot map I am aiming for. I have been working on resolving issues related to decompressing the VCF files into a readable format. Although I am still in the process of resolving this, I plan to complete it soon so that I can proceed with plotting the map. Day 14 (6/27) I have learned how to remove the VCF seal from a file and make the data readable. However, I am facing an issue with my computer's performance as it struggles to handle the entire file. Consequently, it takes about a minute or so for the file to open. Additionally, I recently learned the importance of not prematurely terminating a Jupyter lab session. Unfortunately, I made that mistake and ended up losing the downloaded data, which forced me to redownload everything. Day 15 (6/27) I worked on decompressing all of the files, but unfortunately most didn't decompress successfully which led me to not go through the pain of decompressing all, but usinhg cyvcf instead to read the files for me and decompress it. I went on to copy all of the files into my de.cyverse account and now plan to work on the results of the data. Day 16 (6/28) I worked on making the graphs, data is too large for Jupyter lab to generate. My code has had couple checks and it has been working but since the genetic file is too large it never generates the graph needed but loads forever, i plan to use only small part of the data. Day 17 (6/29) I finally got results, and plan to upadte all of my finding to the website this week and making my poster. Day 18 (7/3) Working on poster, and KEYS assignment! Day 19-22 (7/4-7) Previous result wasn't what i was looking for, there was lack of data that can be interrupted, so I worked to generate a data that people can interactive with and data that includes all samples rather than 100 or less. *Day 23-26 (7/10-13): Worked to fix the issue i had with the data and worked on KEYS assignment and updated my finding on my poster! Day 27 (7-14): Got the geographic map, and finished Working on my poster!","title":"Logbook"},{"location":"myproject/","text":"my project \u00b6 Problem: How diverse are humans based on regions? DNA research has predominantly focused on Europeans, potentially leading to a misrepresentation of non-European populations. This disparity raises concerns about accurately capturing genetic diversity among different ethnic groups. Study: We investigated the human genome using a dataset of 2,504 samples from 26 populations in the 1000 Genomes Project Phase 3 [1]. By analyzing the principal components analysis (PCA), we aim to gain insights into the diversity within these populations and advance our understanding of human genetic variation. Objective: Our goal was to employ large language models to analyze the 1000 Genomes dataset and generate comprehensive data showcasing the diversity based on different regions. We used ChatGPT to help write Python code to analyze genomic data. By addressing the underrepresentation of non-European populations, we aim to contribute to a more inclusive understanding of human genetic variation across diverse populations.","title":"My project"},{"location":"myproject/#my-project","text":"Problem: How diverse are humans based on regions? DNA research has predominantly focused on Europeans, potentially leading to a misrepresentation of non-European populations. This disparity raises concerns about accurately capturing genetic diversity among different ethnic groups. Study: We investigated the human genome using a dataset of 2,504 samples from 26 populations in the 1000 Genomes Project Phase 3 [1]. By analyzing the principal components analysis (PCA), we aim to gain insights into the diversity within these populations and advance our understanding of human genetic variation. Objective: Our goal was to employ large language models to analyze the 1000 Genomes dataset and generate comprehensive data showcasing the diversity based on different regions. We used ChatGPT to help write Python code to analyze genomic data. By addressing the underrepresentation of non-European populations, we aim to contribute to a more inclusive understanding of human genetic variation across diverse populations.","title":"my project"},{"location":"poster/","text":"","title":"Poster"},{"location":"prompts/","text":"Prompting","title":"Prompts"},{"location":"references/","text":"References \u00b6 https://www.internationalgenome.org/data-portal/population","title":"References"},{"location":"references/#references","text":"https://www.internationalgenome.org/data-portal/population","title":"References"}]}